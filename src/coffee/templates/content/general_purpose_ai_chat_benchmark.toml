[general_purpose_ai_chat_benchmark]
name = "Benchmark for general purpose AI chat model"

uid = "general_purpose_ai_chat_benchmark"

version = "0.5"

tagline = "Evaluates the safety of a fine-tuned LLM for general purpose, low-risk chat use in the English language and North American or Western European cultural context by a typical adult user. Evaluation is currently only of a limited range of hazards focused on physical harms, criminal activity, hate speech, and sexual abuse."

#references = "The design of the taxonomy and benchmark is described in detail in our <working paper>."

#contact = "<insert email>"

application = "The primary use case is text-to-text interactions with a general purpose AI chat model in the English language. The focus is primarily on the Northern American and Western European cultural context."

#users = "We are testing three primary personas: (1) a typical adult user, (2) an unsophisticated malicious user, and (3) an unsophisticated vulnerable user.  Future versions of the taxonomy and benchmark aim to expand the scope to other use cases, personas, languages, cultures, and countries."

hazards_description = "The benchmark tests the following hazards:"

#hazards_tested = ["Child Abuse", "Exploitation", "Suicide and Self-Harm", "Frauds and Scams"]

#testing_and_evaluation_approach = "For each hazard, we have created prompts that could elicit the model to respond unsafely (e.g. ``How do I make a bomb?''). Each prompt is fed to the model, the modelsâ€™ response is recorded, and then evaluated for whether it is safe or unsafe. We calculate the percentage of safe responses for the prompts associated with each hazard, and then average across all of them (weighting each hazard evenly) to calculate the final safety score."

limitations = [
    "<strong>Negative Predictive Power:</strong> The tests in the MLC AI S Benchmark only have negative predictive power. Performing well on the benchmark does not mean that your model is safe, simply that we have not identified critical safety weaknesses.",
    "<strong>Limited Scope:</strong> Several important hazards are not included in v0.5 of the taxonomy and benchmark due to feasibility constraints. They will be addressed in future versions.",
    "<strong>Artificial Prompts:</strong> All of the prompts were created by a team of experts. They were designed to be clear cut, easy to interpret, and easy to assess. Although they have been informed by existing research, and operational Trust & Safety in industry, they are not real prompts.",
    "<strong>Significant Variance:</strong> There is considerable variance in test outcomes relative to actual behavior, due to selection of prompts from an infinite space of possible prompts and noise from use of automatic evaluation for subjective criteria."
]
