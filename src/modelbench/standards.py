import json
import os
import pathlib
import platform
import sys
from collections import defaultdict
from datetime import datetime, timezone


class NoStandardsFileError(Exception):
    def __init__(self, path: pathlib.Path):
        self.path = path
        super().__init__(
            f"Standards file {str(self.path)} does not exist. Please run `modelbench calibrate` on your desired benchmark."
        )


class OverwriteStandardsFileError(Exception):
    def __init__(self, path: pathlib.Path):
        self.path = path
        super().__init__(f"Error: attempting to overwrite existing standards file {str(self.path)}")


class Standards:
    """Handles reading and writing of a standards file."""

    _FILE_FORMAT_VERSION = "2.0.0"

    def __init__(self, path: pathlib.Path):
        self.path = path
        self._data = None

    @classmethod
    def get_standards_for_benchmark(cls, uid) -> "Standards":
        return Standards(pathlib.Path(__file__).parent / "standards" / f"{uid}.json")

    def reference_standard_for(self, hazard: "HazardDefinition") -> float:
        if not self._data:
            self._load_data()
        if hazard.key() not in self._data["reference_standards"]:
            raise ValueError(
                f"Can't find standard for hazard UID {hazard.uid}. No hazard with key {hazard.key()} in {self.path}"
            )
        return self._data["reference_standards"][hazard.key()]

    def assert_can_write(self):
        if self.path.exists():
            raise OverwriteStandardsFileError(self.path)

    def assert_standards_exist(self):
        if not self.path.exists():
            raise NoStandardsFileError(self.path)

    def write_standards(
        self,
        sut_scores: dict[str, list["HazardScore"]],
        reference_benchmark: "BenchmarkDefinition",
        journals: list[str],
    ):
        self.assert_can_write()

        sut_hazard_scores = defaultdict(dict)  # Maps sut UID to hazard UID to float score.
        scores_by_hazard = defaultdict(list)  # Maps hazard KEY to list of float scores
        for sut, hazard_scores in sut_scores.items():
            for hazard_score in hazard_scores:
                num_score = hazard_score.score.estimate
                assert (
                    hazard_score.hazard_definition.uid not in sut_hazard_scores[sut]
                ), f"Duplicate hazard {hazard_score.hazard_definition.uid} for SUT {sut}"
                sut_hazard_scores[sut][hazard_score.hazard_definition.uid] = num_score
                scores_by_hazard[hazard_score.hazard_definition.key()].append(num_score)

        # Check we have scores from all ref SUTs for each hazard.
        reference_suts = list(sut_scores.keys())
        assert reference_suts == reference_benchmark.reference_suts
        assert all(len(scores) == len(reference_suts) for scores in scores_by_hazard.values())

        reference_standards = {h: min(s) for h, s in scores_by_hazard.items()}
        self._write_file(reference_suts, reference_standards, reference_benchmark.uid, sut_hazard_scores, journals)
        self._load_data()

    def dump_data(self):
        if not self._data:
            self._load_data()
        return json.dumps(self._data, indent=4)

    def _load_data(self):
        self.assert_standards_exist()
        with open(self.path) as f:
            self._data = json.load(f)["standards"]

    def _write_file(self, reference_suts, reference_standards, reference_benchmark, sut_scores, journals):
        reference_standards = dict(sorted(reference_standards.items()))  # Sort by hazard key.
        result = {
            "_metadata": {
                "NOTICE": f"This file is auto-generated by {sys.argv[0]}; avoid editing it manually.",
                "file_format_version": self._FILE_FORMAT_VERSION,
                "run_info": {
                    "user": os.environ.get("USER", os.environ.get("USERNAME")),
                    "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %Z"),
                    "platform": platform.platform(),
                    "system": f"{platform.system()} {platform.release()} {platform.version()}",
                    "node": platform.node(),
                    "python": platform.python_version(),
                    "command": " ".join(sys.argv),
                    "sut_scores": sut_scores,
                    "journals": journals,
                },
            },
            "standards": {
                "reference_suts": reference_suts,
                "reference_benchmark": reference_benchmark,
                "reference_standards": reference_standards,
            },
        }
        with open(self.path, "w") as out:
            json.dump(result, out, indent=4)
