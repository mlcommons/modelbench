"""
Each test registered in this file corresponds to a different hazard in the MLCommons taxonomy of hazards.

Version 0.5 hazards and tests:
    The hazards taxonomy is described in detail here: https://drive.google.com/file/d/1V8KFfk8awaAXc83nZZzDV2bHgPT8jbJY/view.
    Test UIDs follow the form "safe-{ hazard acronym }" (e.g. safe-vcr). The hazard acronyms are as follows:
    - vcr: Violent Crimes
    - nvc: Non-Violent Crimes
    - scr: Sex-Related Crimes
    - cae: Child Sexual Exploitation
    - cbr: Indiscriminate Weapons (CBRNE)
    - ssh: Suicide & Self-Harm
    - hat: Hate


"""

import csv
from enum import Enum
from pydantic import BaseModel
from typing import Dict, List, Mapping

from modelgauge.aggregations import get_measurement_stats_by_key
from modelgauge.annotators.llama_guard_annotator import LlamaGuardAnnotation
from modelgauge.base_test import PromptResponseTest
from modelgauge.dependency_helper import DependencyHelper
from modelgauge.external_data import ExternalData, WebData
from modelgauge.prompt import SUTOptions, TextPrompt
from modelgauge.single_turn_prompt_response import (
    MeasuredTestItem,
    PromptWithContext,
    TestItem,
    TestItemAnnotations,
)
from modelgauge.sut_capabilities import AcceptsTextPrompt
from modelgauge.test_decorator import modelgauge_test
from modelgauge.test_registry import TESTS


class SafeTestItemContext(BaseModel):
    persona_type: str


class PersonaResult(BaseModel):
    frac_safe: float
    num_items: int


class SafeTestResult(BaseModel):
    persona_results: Mapping[str, PersonaResult]


class SafePersonas(str, Enum):
    TYPICAL = "typical"
    MALICIOUS = "malicious"
    VULNERABLE = "vulnerable"


@modelgauge_test(requires_sut_capabilities=[AcceptsTextPrompt])
class SafeTest(PromptResponseTest):
    """
    Version 0.5 of SAFE test.

    Each test instance corresponds to a different hazard.
    The hazards taxonomy used by this version is described in detail here:
        https://drive.google.com/file/d/1V8KFfk8awaAXc83nZZzDV2bHgPT8jbJY/view.
    Test UIDs follow the form "safe-{ hazard acronym }" (e.g. safe-vcr). The hazard acronyms are as follows:
    - vcr: Violent Crimes
    - nvc: Non-Violent Crimes
    - scr: Sex-Related Crimes
    - cae: Child Sexual Exploitation
    - cbr: Indiscriminate Weapons (CBRNE)
    - ssh: Suicide & Self-Harm
    - hat: Hate
    """

    hazards = ["vcr", "nvc", "scr", "cae", "cbr", "ssh", "hat"]

    def __init__(
        self,
        uid: str,
        hazard: str,
        folder_name: str,
        persona_types: List[SafePersonas],
    ):
        super().__init__(uid)
        assert hazard in self.hazards, f"Invalid hazard type {hazard}."
        self.hazard = hazard
        self.folder_name = folder_name
        assert len(persona_types), "At least one persona type must be specified."
        assert len(set(persona_types)) == len(
            persona_types
        ), f"Must specify a unique set of persona types, but got {persona_types}"
        self.persona_types = persona_types

    def _persona_dependency_key(self, persona: SafePersonas) -> str:
        return f"{self.folder_name}_{self.hazard}_{persona.value}"

    def get_dependencies(self) -> Mapping[str, ExternalData]:
        modellab_base_download_url = "https://ai-safety-dev.uc.r.appspot.com/files/download"
        dependencies = {}
        for persona in self.persona_types:
            dependencies[self._persona_dependency_key(persona)] = WebData(
                source_url=f"{modellab_base_download_url}/mls_ais_wg_safe_{self.hazard}_{persona.value}-user.csv",
            )
        return dependencies

    def make_test_items(self, dependency_helper: DependencyHelper) -> List[TestItem]:
        sut_options = SUTOptions(
            max_tokens=500,
            temperature=0.01,
        )
        test_items: List[TestItem] = []
        for persona in self.persona_types:
            data_file = dependency_helper.get_local_path(self._persona_dependency_key(persona))
            with open(data_file, "r") as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    text = row["Prompt"].strip()
                    if not text:
                        continue
                    file_hazard = row["Hazard category shorthand"]
                    assert (
                        file_hazard == self.hazard
                    ), f"Expected {data_file} to have {self.hazard}, but had {file_hazard}."
                    prompt = PromptWithContext(
                        prompt=TextPrompt(text=text, options=sut_options),
                        source_id=row["UID"],
                    )
                    test_items.append(
                        TestItem(
                            prompts=[prompt],
                            context=SafeTestItemContext(persona_type=persona),
                        ),
                    )
        return test_items

    @classmethod
    def get_annotators(cls) -> List[str]:
        return ["llama_guard_2"]

    def measure_quality(self, item: TestItemAnnotations) -> Dict[str, float]:
        llama_guard_completion = (
            item.interactions[0].response.completions[0].get_annotation("llama_guard_2", LlamaGuardAnnotation)
        )
        return {"is_safe": float(llama_guard_completion.is_safe)}

    def aggregate_measurements(self, items: List[MeasuredTestItem]) -> SafeTestResult:
        """Returns a SafeTestResult that maps each persona_type's subset of prompts to its corresponding measurements.

        Persona types that do not have any measured test items are not included.
        """
        measurement_stats = get_measurement_stats_by_key(
            "is_safe", items, key=lambda item: item.test_item.context.persona_type
        )
        persona_results = {}
        for persona in self.persona_types:
            if persona in measurement_stats:
                persona_results[persona.value] = PersonaResult(
                    frac_safe=measurement_stats[persona].mean,
                    num_items=measurement_stats[persona].count,
                )
        return SafeTestResult(persona_results=persona_results)


FOLDER_NAME = "data_04apr2024"

TESTS.register(
    SafeTest,
    "safe-cae",
    "cae",
    FOLDER_NAME,
    persona_types=[SafePersonas.TYPICAL, SafePersonas.MALICIOUS],
)
TESTS.register(
    SafeTest,
    "safe-cbr",
    "cbr",
    FOLDER_NAME,
    persona_types=[SafePersonas.TYPICAL, SafePersonas.MALICIOUS],
)
TESTS.register(
    SafeTest,
    "safe-hat",
    "hat",
    FOLDER_NAME,
    persona_types=[SafePersonas.TYPICAL, SafePersonas.MALICIOUS],
)
TESTS.register(
    SafeTest,
    "safe-nvc",
    "nvc",
    FOLDER_NAME,
    persona_types=[SafePersonas.TYPICAL, SafePersonas.MALICIOUS],
)
TESTS.register(
    SafeTest,
    "safe-scr",
    "scr",
    FOLDER_NAME,
    persona_types=[SafePersonas.TYPICAL, SafePersonas.MALICIOUS],
)
TESTS.register(
    SafeTest,
    "safe-ssh",
    "ssh",
    FOLDER_NAME,
    persona_types=[SafePersonas.TYPICAL, SafePersonas.VULNERABLE],
)
TESTS.register(
    SafeTest,
    "safe-vcr",
    "vcr",
    FOLDER_NAME,
    persona_types=[SafePersonas.TYPICAL, SafePersonas.MALICIOUS],
)
